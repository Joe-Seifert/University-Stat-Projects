---
title: "lab 4"
author: "Joe Seifert"
date: "2/1/2021"
output: pdf_document
---

```{r}
#guided example 1
#goal: analyze how bias of estimator changes as the n increases
#last week we proved that s^2 is always unbiased for sigma^2
#all three problems are similar so use same protocol
sampler<-function(n){
  x<-rnorm(n,100,5)
  stilde2<-sum((x-mean(x))^2)/length(x)
  return(stilde2)
}#mean 100 and SD 5

ns<-c(2,5,10,20,50,100,200,300,400,500)
results<-replicate(10000,unlist(lapply(ns,sampler)))
#lapply returns a list of the same length as the original argument (in this case ns is length of 10 elements)
#unlist transforms the format from a list to a vector
#we have 10,000 vectors of length 10 instead of 10,000 lists of length 10


#what does apply() do? --- look in the help section
#function applied to the margin of a matrix: 1=the rows, 2=the colums
#in this case we are applying the function mean to the rows of a matrix

mean_results<-apply(results,1,mean)
#apply the function mean, to the row (1) of the matrix results


#variance true value = SD^2 = 5^2 = 25

#coding percent error
percent_error<-abs((mean_results-(5^2))/(5^2))

plot(ns,percent_error,type="l",ylab="percent error",xlab="sample size")
#that type 1 means we are using a line

#the bias seems to decay exponentially, there is definitely a point at which increasing the sample does not have high return (looks like that point is about 100). until that point, increasing the sample size significantly decreases the bias of the estimator, which can be seen through the decrease in the percent error


#problem 2
#now we have a continuous uniform distribution rather than Normal

sampler<-function(n){
  x<-runif(n,0,1)
  max_x<-max(x)
  return(max_x)
}

ns<-c(2,5,10,20,50,100,200,300,400,500)
results<-replicate(10000,unlist(lapply(ns,sampler)))
#this is the monte carlo done on all n=c()

max_results<-apply(results,1,mean)
#calculating the mean of the max values for each of the ns values and storing means as a vector
#then we can calculate the avg bias of the max

percent_error<-abs((max_results-(1))/(1))

plot(ns,percent_error,type="l",ylab="percent error",xlab="sample size")

#the percent error decreases with sample size, converging at 0, suggesting that the estimator becomes less biased as we collect more data

#problem 3
sampler<-function(n){
  x<-runif(n,0,1)
  twoxbar<-2*mean(x)
  return(twoxbar)
}

ns<-c(2,5,10,20,50,100,200,300,400,500)
results<-replicate(10000,unlist(lapply(ns,sampler)))
#this is the monte carlo done on all n=c()
#we create a matrix of the results

mean_results<-apply(results,1,mean)

percent_error<-abs((mean_results-(1))/(1))

plot(ns,percent_error,type="l",ylab="percent error",xlab="sample size")

#like in problem 1, we see exponential decay in the percent error, indicating that the estimator is unbiased.  However, the bottom of the decay is still at 99.35%.

#point proven in the lab: even if the estimator is initially biased, as n increases, the percent error will decrease, meaning that the estimator is becoming LESS biased.
```

